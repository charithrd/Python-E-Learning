{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charithrd/Python-E-Learning/blob/main/UD_T6_text_analytics_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kVKW-UkJN20"
      },
      "source": [
        "# Text Analytics Fundamentals\n",
        "\n",
        "## Learning Objectives\n",
        "* Understand why text preprocessing is essential for analysis\n",
        "* Apply core text preprocessing techniques\n",
        "* Perform basic text analysis using Python\n",
        "* Gain hands-on experience with NLTK library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKd_pv2HJN22"
      },
      "source": [
        "## Part 1: Why Text Preprocessing?\n",
        "\n",
        "### The Text Analysis Challenge\n",
        "\n",
        "Consider these challenges with raw text data:\n",
        "* Inconsistent formatting (uppercase/lowercase)\n",
        "* Punctuation and special characters\n",
        "* Common words that don't add meaning (\"the\", \"and\", \"is\")\n",
        "* Different forms of the same word (\"run\", \"running\", \"ran\")\n",
        "\n",
        "Let's see a practical example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QxIn7ESJN23",
        "outputId": "1d6e753f-f80f-4072-979b-b3d2cdae648d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet nltk\n",
        "!pip install --quiet langdetect\n",
        "!pip install --quiet translate\n",
        "!pip install --quiet deep_translator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "from langdetect import detect\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "9nOw9DUWKuYf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e2u5uivJN24",
        "outputId": "30ef0b8a-35b0-4c69-f24c-a6a6ad8a0deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw feedback: \n",
            "TOLLES Produkt!!! Ich benutze es jetzt seit drei Monaten ...\n",
            "Der Kundenservice war wirklich hilfreich :)\n",
            "Kann ich nur weiterempfehlen!!!\n",
            "\n",
            "Raw text contains:\n",
            "- Mixed case: True\n",
            "- Punctuation: True\n",
            "- Special characters: True\n",
            "- Multiple lines: True\n"
          ]
        }
      ],
      "source": [
        "# Raw customer feedback example\n",
        "raw_feedback = \"\"\"\n",
        "TOLLES Produkt!!! Ich benutze es jetzt seit drei Monaten ...\n",
        "Der Kundenservice war wirklich hilfreich :)\n",
        "Kann ich nur weiterempfehlen!!!\n",
        "\"\"\"\n",
        "\n",
        "print(\"Raw feedback:\", raw_feedback)\n",
        "\n",
        "print(\"Raw text contains:\")\n",
        "print(\"- Mixed case:\", any(c.isupper() for c in raw_feedback))\n",
        "print(\"- Punctuation:\", any(c in \"!?.,\" for c in raw_feedback))\n",
        "print(\"- Special characters:\", \":)\" in raw_feedback)\n",
        "print(\"- Multiple lines:\", \"\\n\" in raw_feedback)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw customer feedback example\n",
        "raw_feedback = \"\"\"\n",
        "TOLLES Produkt!!! Ich benutze es jetzt seit drei Monaten ...\n",
        "Der Kundenservice war wirklich hilfreich :)\n",
        "Kann ich nur weiterempfehlen!!!\n",
        "\"\"\"\n",
        "\n",
        "# Detect language\n",
        "language = detect(raw_feedback)\n",
        "\n",
        "# Translate to English\n",
        "translated_feedback = GoogleTranslator(source='auto', target='en').translate(raw_feedback)\n",
        "\n",
        "def has_upper(text):\n",
        "    return any(c.isupper() for c in text if c.isalpha())\n",
        "\n",
        "def has_punctuation(text):\n",
        "    # Check for a range of Unicode punctuation\n",
        "    return any(unicodedata.category(c).startswith('P') for c in text)\n",
        "\n",
        "def has_special_characters(text):\n",
        "    return any(c in \":)\" for c in text)\n",
        "\n",
        "print(\"Raw feedback:\", raw_feedback)\n",
        "print(\"Detected language:\", language)\n",
        "print(\"Translated (English):\", translated_feedback)\n",
        "print(\"\\nRaw text contains:\")\n",
        "print(\"- Mixed case:\", has_upper(raw_feedback))\n",
        "print(\"- Punctuation:\", has_punctuation(raw_feedback))\n",
        "print(\"- Special characters:\", has_special_characters(raw_feedback))\n",
        "print(\"- Multiple lines:\", \"\\n\" in raw_feedback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG85rVxLKznj",
        "outputId": "539410e8-ec22-4d10-d0e7-65921ec67f3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw feedback: \n",
            "TOLLES Produkt!!! Ich benutze es jetzt seit drei Monaten ...\n",
            "Der Kundenservice war wirklich hilfreich :)\n",
            "Kann ich nur weiterempfehlen!!!\n",
            "\n",
            "Detected language: de\n",
            "Translated (English): Great product !!! I've been using it for three months now ...\n",
            "Customer service was really helpful :)\n",
            "I can only recommend it !!!\n",
            "\n",
            "Raw text contains:\n",
            "- Mixed case: True\n",
            "- Punctuation: True\n",
            "- Special characters: True\n",
            "- Multiple lines: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFuWZW_iJN25"
      },
      "source": [
        "## Part 2: Text Preprocessing Steps\n",
        "\n",
        "Let's set up our environment first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRakFdDCJN25",
        "outputId": "96a517f6-2da6-4c7e-c0bf-50b6d9432120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment ready!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"Environment ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcwS4rWcJN25"
      },
      "source": [
        "### 2.1 Tokenisation\n",
        "\n",
        "Tokenisation breaks text into smaller pieces (tokens) - either sentences or words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8aOcgcmJN25",
        "outputId": "5f7ec9cb-f2c6-48e7-adec-2d2216056c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "1. The product exceeded my expectations!\n",
            "2. Customer service was excellent.\n",
            "3. Would recommend.\n",
            "\n",
            "Words: ['The', 'product', 'exceeded', 'my', 'expectations', '!', 'Customer', 'service', 'was', 'excellent', '.', 'Would', 'recommend', '.']\n",
            "Number of words: 14\n"
          ]
        }
      ],
      "source": [
        "# Example text\n",
        "text = \"The product exceeded my expectations! Customer service was excellent. Would recommend.\"\n",
        "\n",
        "# Sentence tokenisation\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sent}\")\n",
        "\n",
        "# Word tokenisation\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWords:\", words)\n",
        "print(f\"Number of words: {len(words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwotxkaRJN25"
      },
      "source": [
        "### 2.2 Case Normalisation\n",
        "\n",
        "Converting text to consistent case helps with analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vg6MLi9JN25",
        "outputId": "d6eb5731-f4b6-402f-ea41-40e0a4d37f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The PRODUCT was AMAZING and I LOVED it!\n",
            "Normalized: the product was amazing and i loved it!\n",
            "\n",
            "Without normalization:\n",
            "\"PRODUCT\" == \"product\": False\n",
            "\n",
            "With normalization:\n",
            "\"PRODUCT\".lower() == \"product\".lower(): True\n"
          ]
        }
      ],
      "source": [
        "# Mixed case example\n",
        "mixed_text = \"The PRODUCT was AMAZING and I LOVED it!\"\n",
        "\n",
        "normalized = mixed_text.lower()\n",
        "print(\"Original:\", mixed_text)\n",
        "print(\"Normalized:\", normalized)\n",
        "\n",
        "# Why this matters\n",
        "print(\"\\nWithout normalization:\")\n",
        "print('\"PRODUCT\" == \"product\":', \"PRODUCT\" == \"product\")\n",
        "print('\\nWith normalization:')\n",
        "print('\"PRODUCT\".lower() == \"product\".lower():', \"PRODUCT\".lower() == \"product\".lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pk36icbJN25"
      },
      "source": [
        "### 2.3 Stop Word Removal\n",
        "\n",
        "Stop words are common words that usually don't add meaning to our analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHxjy2KKJN26",
        "outputId": "27256d4a-2db2-4978-e7d5-8e96f6cdf2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['the', 'product', 'is', 'very', 'good', 'and', 'i', 'would', 'recommend', 'it', 'to', 'others']\n",
            "After removing stop words: ['product', 'good', 'would', 'recommend', 'others']\n",
            "\n",
            "Reduced from 12 to 5 words\n"
          ]
        }
      ],
      "source": [
        "# Get English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example text\n",
        "text = \"The product is very good and I would recommend it to others\"\n",
        "words = word_tokenize(text.lower())\n",
        "\n",
        "# Remove stop words\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"After removing stop words:\", filtered_words)\n",
        "print(f\"\\nReduced from {len(words)} to {len(filtered_words)} words\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "print(\" \". join(filtered_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlANPCC7PDLU",
        "outputId": "c458483f-3b3d-4647-9834-123eec946c82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The product is very good and I would recommend it to others\n",
            "product good would recommend others\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOr2LuV9JN26"
      },
      "source": [
        "### 2.4 Stemming and Lemmatisation\n",
        "\n",
        "These techniques help find the root form of words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTZ2NwppJN26",
        "outputId": "ba7f6777-2ad6-4109-8431-11320accada6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original\tStem\t\tLemma\n",
            "----------------------------------------\n",
            "running\trun\t\trunning\n",
            "runs\trun\t\trun\n",
            "ran\tran\t\tran\n",
            "easily\teasili\t\teasily\n",
            "playing\tplay\t\tplaying\n",
            "better\tbetter\t\tbetter\n",
            "good\tgood\t\tgood\n"
          ]
        }
      ],
      "source": [
        "# Initialize stemmers and lemmatisers\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"playing\", \"better\", \"good\"]\n",
        "\n",
        "# Apply stemming and lemmatisation\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Compare results\n",
        "print(\"Original\\tStem\\t\\tLemma\")\n",
        "print(\"-\" * 40)\n",
        "for original, stem, lemma in zip(words, stems, lemmas):\n",
        "    print(f\"{original}\\t{stem}\\t\\t{lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTI9MsExJN26"
      },
      "source": [
        "#### What is the differnece between stemming and lemmatization?\n",
        "\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form — generally a written word form. Example: \"running\" -> \"run\". Stemming makes use of an algorithmic approach, which allows for the reduction of words to their root form.\n",
        "\n",
        "Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Example: \"better\" -> \"good\". Lemmatisation makes use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
        "\n",
        "| Aspect | Stemming | Lemmatisation |\n",
        "|--------|----------|---------------|\n",
        "| **Definition** | Removes prefixes/suffixes using rules | Converts to dictionary base form |\n",
        "| **Output** | Can produce non-dictionary words | Always produces valid dictionary words |\n",
        "| **Speed** | Fast | Slower |\n",
        "| **Accuracy** | Lower | Higher |\n",
        "| **Example 1** | running → run | running → run |\n",
        "| **Example 2** | better → bet | better → good |\n",
        "| **Example 3** | easily → easili | easily → easy |\n",
        "| **Use Case** | Search engines, large-scale processing | Text analysis, NLP tasks |\n",
        "| **Resource Use** | Minimal (rule-based) | Higher (needs dictionary) |\n",
        "| **Context Aware** | No | Yes |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlE0r_BkJN26"
      },
      "source": [
        "## Part 3: Basic Text Analysis (10 minutes)\n",
        "\n",
        "Let's apply what we've learned to analyze some text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j7KWzD7JN26"
      },
      "outputs": [],
      "source": [
        "def analyse_text(text):\n",
        "    \"\"\"Basic text analysis function\"\"\"\n",
        "    # Normalise the text\n",
        "    normalised_text = text.lower()\n",
        "\n",
        "    # Tokenise the normalised text\n",
        "    words = word_tokenize(normalised_text)\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    words = [word for word in words\n",
        "             if word not in stop_words and word.isalnum()]\n",
        "\n",
        "    # Get word frequencies\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    return {\n",
        "        'total_words': len(words),\n",
        "        'unique_words': len(set(words)),\n",
        "        'most_common': word_freq.most_common(5)\n",
        "    }\n",
        "\n",
        "# Example text\n",
        "customer_reviews = \"\"\"\n",
        "Great product, really helpful for data analysis. The visualisation features are amazing.\n",
        "Easy to use and great support from the team. Would recommend for data science projects.\n",
        "Data visualisation has never been easier. Great tool for analysis.\n",
        "\"\"\"\n",
        "\n",
        "results = analyse_text(customer_reviews)\n",
        "\n",
        "print(\"Analysis Results:\")\n",
        "print(f\"Total words: {results['total_words']}\")\n",
        "print(f\"Unique words: {results['unique_words']}\")\n",
        "print(\"\\nMost common words:\")\n",
        "for word, count in results['most_common']:\n",
        "    print(f\"- {word}: {count} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hDTNjf7JN26"
      },
      "source": [
        "### Exercise (10 minutes)\n",
        "\n",
        "Try analysing some text from your own organisation (e.g., customer feedback, product documentation, or internal communications). Use the template below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkOA6kAXJN27"
      },
      "outputs": [],
      "source": [
        "# Your text here\n",
        "your_text = \"\"\"\n",
        "Replace this with your own text\n",
        "\"\"\"\n",
        "\n",
        "# Analyze it\n",
        "your_results = analyse_text(your_text)\n",
        "\n",
        "# Print results\n",
        "print(\"Your Text Analysis:\")\n",
        "print(f\"Total words: {your_results['total_words']}\")\n",
        "print(f\"Unique words: {your_results['unique_words']}\")\n",
        "print(\"\\nMost common words:\")\n",
        "for word, count in your_results['most_common']:\n",
        "    print(f\"- {word}: {count} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNE7p759JN27"
      },
      "source": [
        "### Summary\n",
        "\n",
        "In this session, we've covered:\n",
        "1. Why text preprocessing is necessary\n",
        "2. Key preprocessing techniques:\n",
        "   - Tokenisation\n",
        "   - Case normalisation\n",
        "   - Stop word removal\n",
        "   - Stemming and lemmatisation\n",
        "3. Basic text analysis methods\n",
        "\n",
        "### Next Steps\n",
        "- Explore more advanced text analysis techniques\n",
        "- Learn about sentiment analysis\n",
        "- Practice with real-world text data\n",
        "\n",
        "### Additional Resources\n",
        "- [NLTK Documentation](https://www.nltk.org/)\n",
        "- [Text Analytics with Python](https://www.nltk.org/book/)\n",
        "- [Practical Text Mining with Python](https://realpython.com/nltk-nlp-python/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}